{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "* we look to intellignet software to automate routine labor, understand speech or images, make diagnoses in medicine and support basic scientific research.\n",
    "* Early days: problems that can be described by a list of formal, mathematical rules.\n",
    "* Now: allow computers to learn from experience and understand the world in terms of a hierachy of concepts. We call this approach to __AI deep learning__.\n",
    "\n",
    "* __knowledge base__ approach to artificial intelligence: hard-code knowledge about the world in formal languages.\n",
    "\n",
    "* __machine learning__ acquire their own knowledge.\n",
    "\n",
    "* __logistic regression__, __naive Bayes__: simple ML algorithms, heavily depend on the representation of the data they are given (human made _features_ are given)\n",
    "\n",
    "* Many artificial intelligence tasks can be solved by designing the right set of features to extract for that task, then providing these features to a simple machine learning algorithm.\n",
    "\n",
    "* __representation learning__: Use machine learning to discover not only the mapping from representation to output but also the representation itself.\n",
    "\n",
    "* Goal: separate factors of variation, source of influence. Most applications require us to disentangle the factors of variation and discard the ones that we do not care about.\n",
    "\n",
    "* __Deep learning__ solves this central problem in representation learning by introducing representations that are expressed in terms of other, simpler representations.\n",
    "\n",
    "* __multilayer perceptron (MLP)__: A mathematical function, formed by composing many simpler functions, mapping some set of input values to output values.We can think of each application of a different mathematical function as providing a new representation of the input.\n",
    "\n",
    "* Networks with greater depth can execute more instructions in sequence. Sequential instructions offer great power because later instructions can refer back to the results of earlier instructions.\n",
    "\n",
    "* there is no single correct value for the depth of an architecture, nor is there a consensus about how much depth a model requires to qualify as “deep.”\n",
    "![](./pngs/1.0.1.png)\n",
    "![](./pngs/1.0.2.png)\n",
    "![](./pngs/1.0.3.png)\n",
    "![](./pngs/1.0.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.1 Who Should Read This Book?\n",
    "1. Universty students.\n",
    "2. Software Engineers.\n",
    "\n",
    "\n",
    "* Part1: basic mathematical tools\n",
    "* Part2: the most established deep learning algorithms\n",
    "* Part3: more speculative ideas that are widely believed to be important for future research in deep learning\n",
    "\n",
    "![](./pngs/1.1.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Historical Trends in Deep Learning\n",
    "* long and rich history, but has gone by many names.\n",
    "* more useful as the amount of available training data has increased.\n",
    "* DL models have grown in size over time as computer infrastructure imporved.\n",
    "* DL has solved increasingly complicated applications with increasing accuracy over time.\n",
    "### 1.2.1 The Many Names and Changing Fortunes of Neural Networks\n",
    "* __cybernetics__: 1940s-1960s, development of theories of biological learning and implementations of the first models such as the perceptron allowing the training of a single neuron.\n",
    "* __connectionism__: 1980s-1990s, back-propagation to train a neural network with one or two hidden layers.\n",
    "* __deep learning__: 2006-\n",
    "* linear model:\n",
    "$f(x,w)=x_1w_1+...+x_nw_n$\n",
    "* w set by human in early stage.\n",
    "* adaptive linear element (ADALINE): first model that could learn the weights defining the categories given examples of inputs.\n",
    "* stochastic gradient descent\n",
    "* Linear model limitations: cannot learn the XOR function: $f([0,1],w)=1,f([1,0],w)=1,f([1,1],w)=0,f([0,0],w)=0$\n",
    "\n",
    "### 1.2.2 Increasing Dataset Sizes\n",
    "* The age of “Big Data” has made machine learning much easier because the key burden of statistical estimation—generalizing well to new data after observing only a small amount of data—has been considerably lightened.\n",
    "\n",
    "### 1.2.3 Increasing Model Sizes\n",
    "* Since the introduction of hidden units, artificial neural networks have doubled in size roughly every 2.4 years. Unless new technologies allow faster scaling, artificial neural networks will not have the same number of neurons as the human brain until at least the 2050s.\n",
    "\n",
    "### 1.2.4 Increasing Accuracy, Complexity and Read-World Impact\n",
    "* Image recognition\n",
    "* Speech recognition\n",
    "* Recurrent neural networks, such as the LSTM sequence model are now used to model relationships between sequences and other sequences rather than just fixed inputs.\n",
    "* Another crowning achievement of deep learning is its extension to the domain of __reinforcement learning__. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
