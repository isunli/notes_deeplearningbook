{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of a feedforward network is to approximate some function $f^∗$.  \n",
    "These models are called feedforward because information flows through the\n",
    "function being evaluated from x, through the intermediate computations used to\n",
    "define $f$ , and finally to the output $y$.  \n",
    "\n",
    "The final layer of a feedforward network is called the _output layer_.\n",
    "\n",
    "The training examples specify directly what the output layer must do at each point $x$; it must produce a value that is close to $y$. The behavior of the other layers is not directly specified by the training data.  \n",
    "\n",
    "Instead, the learning algorithm must decide how to use these layers to best implement an approximation of $f^∗$.  \n",
    "\n",
    "\n",
    "Rather than thinking of the layer as representing a single vector-to-vector function, we can also think of the layer as consisting of many units that act in parallel, each representing a vector-to-scalar function.  \n",
    "\n",
    "We can add $\\phi (x)$ to linear functin to procide a set of features describing $x$, or provide a new representation for $x$.  \n",
    "1. infinite-dimensional, but generalization to the test set often remains poor. usually based only on the principle of local smoothness and do not encode enough prior information to solve advanced problems.  \n",
    "2. manually engineer $\\phi$  \n",
    "3. learn $\\phi$, The advantage is that the human designer only needs to find the right general function family rather than finding precisely the right function.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Example: Learning XOR\n",
    "The XOR function ('exclusive or') is an operation on two bindary values, $x_1$ and $x_2$. When exactly one of these binary values is equal to 1, the XOR function returns 1. Otherwise, it returns 0.We will focus on four points $X = {[0,0]^T,[0,1]^T,[1,0]^T,[1,1]^T}$. \n",
    "MSE loss function:  \n",
    "$$\n",
    "J(\\theta)={1\\over 4} \\sum_{x \\in X} (f^*(x)-f(x;\\theta))^2\n",
    "$$  \n",
    "The linear model give us $f^* = 0.5$ which is not able to represent the XOR funciton.  \n",
    "We use a simple feedforward neural network for this, and we must use a nonlinear function to describe the features.  \n",
    "The activation function _g_ is typically chosen to be a function that is applied element-wise, ReLU is g(z) = max{0,z}. So the whole network is:\n",
    "$$\n",
    "f(x,W,c,w,b) = w^T max(0,W^T x + c)+b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6.2 Gradient-Based Learning\n",
    "The largest difference between the linear models we have seen so far and neural networks is that the nonlinearity of a neural network causes most interesting loss functions to become non-convex.   \n",
    "Stochastic gradient descent applied to non-convex loss functions has no such convergence guarantee, and is sensitive to the values of the initial parameters.  \n",
    "For feedforward neural networks, it is important to initialize all weights to small random values. The biases may be initialized to zero or to small positive values. \n",
    "### 6.2.1 Cost Functions\n",
    "The total cost function used to train a neural network will often combine one of the primary cost functions described here with a regularization term.  \n",
    "#### 6.2.1.1 Learning Conditional Distributions with Maximum Likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
