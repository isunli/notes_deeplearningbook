{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter2 Linear Algeba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Scalars, Vectors, Matrices and Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* __Scalars__: single number, $\\textit s$\n",
    "* __Vectors__: an array of numbers, $\\textbf x$,fisrt element: $\\textit x_1$. Point in space.\n",
    "* __Matrics__: 2-D array of numbers, each element is identified by two indicies instead of just one. $\\textbf A, \\textbf A_{i,:},\\textbf A_{:,i}, A_{m,n}$\n",
    "* __Tensors__: array with more than two axes. $A_{i,j,k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose: the transpose of a matrix is the mirror image of the matrix   across a diagonal line, caller the _main diagonal_.  \n",
    "The tanspose of a vector is a matrix with only one row.  \n",
    "Scalar is its own transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If matrix A is \n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "Transpose A is \n",
      "[[0 3]\n",
      " [1 4]\n",
      " [2 5]]\n",
      "Aij == (A^T)ji\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "A = np.reshape(range(6),(2,3))\n",
    "print('If matrix A is ')\n",
    "print(A)\n",
    "print('Transpose A is ')\n",
    "AT=A.transpose()\n",
    "print(AT)\n",
    "\n",
    "print('Aij == (A^T)ji')\n",
    "print(A[1,2]==AT[2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Add: add corresponding elements  \n",
    "D = a · B + c where Di,j =a·Bi,j +c  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "+\n",
      "[[ 7  8  9]\n",
      " [10 11 12]]\n",
      "is\n",
      "[[ 7  9 11]\n",
      " [13 15 17]]\n"
     ]
    }
   ],
   "source": [
    "A = np.reshape(range(6),(2,3))\n",
    "B = np.reshape(range(7,13),(2,3))\n",
    "print(A)\n",
    "print('+')\n",
    "print(B)\n",
    "print('is')\n",
    "print(A+B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C=A+b$  \n",
    "The vector $b$ is added to each row of the matrix. This is called __broadcastring__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "+\n",
      "[ 1.  1.  1.]\n",
      "is\n",
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]]\n"
     ]
    }
   ],
   "source": [
    "C = np.ones(3)\n",
    "print(A)\n",
    "print('+')\n",
    "print(C)\n",
    "print('is')\n",
    "print(A+C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Multiplying Matrices and Vectors\n",
    "The matrix product of matrices A and B is a third matrix C  \n",
    "A must have the same number of columns as B has rows.  \n",
    "If A is of shape m × n and B is of shape n × p, then C is of shape m × p.  \n",
    "$\\textbf {C = AB}$  \n",
    "$C_{i,j}=\\sum_{k}A_{i,k}B_{k,j}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "multiply\n",
      "[[ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]]\n",
      "is\n",
      "[[ 31  34]\n",
      " [112 124]]\n"
     ]
    }
   ],
   "source": [
    "A = np.reshape(range(6),(2,3))\n",
    "B = np.reshape(range(7,13),(3,2))\n",
    "print(A)\n",
    "print('multiply')\n",
    "print(B)\n",
    "print('is')\n",
    "print(np.matmul(A,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Element-wise product__ or __Hadamard product__, and is denoted as $\\textbf A\\odot \\textbf B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "hadamard multiply\n",
      "[[ 7  8  9]\n",
      " [10 11 12]]\n",
      "is\n",
      "[[ 0  8 18]\n",
      " [30 44 60]]\n"
     ]
    }
   ],
   "source": [
    "A = np.reshape(range(6),(2,3))\n",
    "B = np.reshape(range(7,13),(2,3))\n",
    "print(A)\n",
    "print('hadamard multiply')\n",
    "print(B)\n",
    "print('is')\n",
    "print(A*B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __dot product__ between two vectors $x$ and $y$ of the same dimensionality is the matrix product $x^T y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n",
      "dot\n",
      "[ 7  8  9 10 11 12]\n",
      "is\n",
      "160\n",
      "==\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "A = np.array(range(6))\n",
    "B = np.array(range(7,13))\n",
    "print(A)\n",
    "print('dot')\n",
    "print(B)\n",
    "print('is')\n",
    "print(np.matmul(A.transpose(),B))\n",
    "print('==')\n",
    "print(np.dot(A,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]\n",
      " [ True  True]]\n"
     ]
    }
   ],
   "source": [
    "A = np.reshape(range(6),(2,3))\n",
    "B = np.reshape(range(7,13),(3,2))\n",
    "C = np.reshape(range(13,19),(3,2))\n",
    "print(np.matmul(A,B+C)==np.matmul(A,B)+np.matmul(A,C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True]\n",
      " [ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "A = np.reshape(range(6),(2,3))\n",
    "B = np.reshape(range(7,13),(3,2))\n",
    "C = np.reshape(range(13,19),(2,3))\n",
    "print(np.matmul(A,np.matmul(B,C))==np.matmul(np.matmul(A,B),C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO commutative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False]\n",
      " [False False]]\n"
     ]
    }
   ],
   "source": [
    "A = np.reshape(range(4),(2,2))\n",
    "B = np.reshape(range(5,9),(2,2))\n",
    "print(np.matmul(A,B)==np.matmul(B,A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product between two\n",
    "vectors is commutative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "A = np.array(range(4))\n",
    "B = np.array(range(5,9))\n",
    "print(np.dot(A,B)==np.dot(B,A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transpose of a matrix product has a simple form:\n",
    "${(AB)^T=B^T A^T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]\n",
      " [ True  True]]\n"
     ]
    }
   ],
   "source": [
    "A = np.reshape(range(6),(2,3))\n",
    "B = np.reshape(range(7,13),(3,2))\n",
    "print(np.matmul(A,B).transpose()==np.matmul(B.transpose(),A.transpose()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A system of linear equation in linear algebra:  \n",
    "$$\n",
    "\\textbf {Ax=b}\\\\\n",
    "\\textbf A \\in \\mathbf{R}^{m*n}\\\\\n",
    "\\textbf x \\in \\mathbf{R}^{n}\\\\\n",
    "$$\n",
    "This is same as:  \n",
    "\n",
    "$$\n",
    "A_{1,1}x_1+A_{2,1}x_2+...+A_{1,n}x_n=b_1\\\\\n",
    "...\\\\\n",
    "A_{1,m}x_m+A_{2,m}x_m+...+A_{m,n}x_n=b_m\n",
    "$$  \n",
    "Matrix-vector product notation provides a more compact representation for equations of this form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.3 Identity and Inverse Matrices\n",
    "* __identity matrix__: matrix that does not change any vector when we multiply that vector by that matrix. We denote the identity matrix that preserves n-dimensional vectors as $\\textit I_n \\in R^{n*n}$ and  \n",
    "$ \\forall \\textbf x \\in R^{n}, \\textit I_n x = x$  \n",
    "* __matrix inverse__: $\\textbf A^{-1} \\textbf A=\\textbf I_n$  \n",
    "So the equation above can solved by: $ \\textbf x =\\textbf A^{-1} \\textbf b$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "[-4.5  5. ]\n",
      "OR\n",
      "[-4.5  5. ]\n"
     ]
    }
   ],
   "source": [
    "print(np.identity(5))\n",
    "A = np.reshape(range(4),(2,2))\n",
    "b = np.array([5,6])\n",
    "print(np.matmul(np.linalg.inv(A),b))\n",
    "print('OR')\n",
    "print(np.linalg.solve(A,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $A^{-1}$ exists, several different algorithms exist for finding it in closed form. In practice, $A^{-1}$ can be represented with only limited precision on a digital computer, algorithms that make use of the value $b$ can usually obtain more accurate estimates of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Linear Dependence and Span\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If order for $A^{-1}$ to exist, linear equations must have exactly one solution for every value of $b$.  \n",
    "We can think of the columns of $A$ as specifying different directions we can travel from the origin, and determine how many ways there are of reaching $b$.  \n",
    "Each element of $x$ specifies how far we should travel in each of these directions. (linear combination)  \n",
    "The span of a set of vectors is the set of all points obtainable by linear combination of the original vectors.  \n",
    "\n",
    "Determining whether $Ax = b$ has a solution thus amounts to testing whether\n",
    "$b$ is in the span of the columns of A. This particular span is known as the \n",
    "_column space_ or the _range_ of A.  \n",
    "We therefore require that the column space of $A$ be all of $b \\in R^m$  \n",
    "\n",
    "Consider a 3 × 2 matrix. The target $b$ is 3-D, but $x$ is only 2-D, so modifying the value of $x$ at best allows us to trace out a 2-D plane within $R^3$. The equation has a solution if and only if $b$ lies on that plane.   \n",
    "\n",
    "$n>m$ is a necessary condition for every point to have a solution.   \n",
    "A set of vectors is _linearly independent_ if no vector in the set is a linear combination of the other vectors.   \n",
    "\n",
    "The matrix must contain at least one set of _m_ linearly independent columns. This condition is both necessary and sufficient.  \n",
    "\n",
    "In order for the matrix to have an inverse, we additionally need to ensure that\n",
    "Equation has at most one solution for each value of b. To do so, we need to ensure that the matrix has at most m columns. Otherwise there is more than one way of parametrizing each solution.  \n",
    "Together, this means that the matrix must be square, that is, we require that\n",
    "$m = n$ and that all of the columns must be linearly independent. A square matrix with _linearly dependent_ columns is known as _singular_.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, the size of a vector, $L^p$ norm is given by:  \n",
    "$$\n",
    "||x||_p = (\\sum_i |x_i|^p)^{1/p} \\\\\n",
    "for\\ p \\in R, p>=1\n",
    "$$  \n",
    "Norms are functions mapping vectors to non-negative values.  \n",
    "On an intuitive level, the norm of a vector $x$ measures the distance from the origin to the points $x$. More rigorously, a norm is any function _f_ that satisies the following properties:  \n",
    "* $f(x) = 0 \\rightarrow x = 0  $\n",
    "* $f(x+y)<= f(x) + f(y) $ \n",
    "* $\\forall \\alpha \\in R, f(\\alpha x)=|\\alpha|f(x)$  \n",
    "\n",
    "The $L^2$ norm is known as the _Euclidean norm_, it is often denoted simply as $||x||$, the squared $L^2$ norm can be calculated as $x^Tx$.  \n",
    "The derivatives of __squared__ $L^2$ with respect to each element of $x$ each depend only on the corresponding element of $x$, while all of the derivatives of the $L^2$ norm depend on the entir vector.  \n",
    "The __squared__ $L^2$ increases very slowly near the origin. In the case we need to discriminate between 0 and samll but nonzero, we use function grows at the same rate in all locations: the $L^1$ norm.  \n",
    "$$\n",
    "||x||_1=\\sum_i |x_i|\n",
    "$$  \n",
    "One other norm that commonly arises in machine learning is the $L^\\infty$, also known as the _max norm_. This norm simplifies to the absolute value of the element with the largest magnitude in the vector:  \n",
    "$$||x||_\\infty = max_i|x_i|$$\n",
    "Size of a matrix: _Frobenius norm_, $||A||_F=\\sqrt{\\sum_{i,j}A^2_{i,j}}$, analogous to the $L^2$ norm of a vector.  \n",
    "The dot product of two vectors can be rewritten in terms of norms:  \n",
    "$$x^Ty=||x||_2||y||_2cos(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Special Kinds of Matrices and Vectors\n",
    "* _Diagonal_ matrix: identity matrix, $D_{i,j}=0$ for all $i!=j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 2, 0],\n",
       "       [0, 0, 3]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$diag(v)x = v\\odot x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24 30 36]\n",
      "is not same as\n",
      "[[ 0  2  6]\n",
      " [ 3  8 15]\n",
      " [ 6 14 24]]\n",
      "???\n"
     ]
    }
   ],
   "source": [
    "A = np.diag([1,2,3])\n",
    "B = np.reshape(range(9),(3,3))\n",
    "A = np.array([1,2,3])\n",
    "print(np.matmul(A,B))\n",
    "print('is not same as')\n",
    "print(A*B)\n",
    "print('???')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The inverse exists only if every diagonal entry is nonzero, and in that case, $diag(v)^{−1} = diag([1/v_1,...,1/v_n ]^T)$  \n",
    "It is possible to construct a rectangular diagonal matrix. Non-square diagonal matrices do not have inverses but it is still possible to multiply by them cheaply.  \n",
    "The product $Dx$ will involve scaling each element of $x$, and either concatenating some zeros to the result if $D$ is taller than it is wide or discarding some of the last elemetns of the vector if $D$ is wider than it is tall.  \n",
    "\n",
    "_Symmetric matrix_:\n",
    "$$A=A^T$$  \n",
    "Symmetric matrices often arise when the entries are generated by some function of two arguments that does not depend on the order of the arguments. (like distance)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "print(np.diag([1,2,3])==np.diag([1,2,3]).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Unit Vector_  \n",
    "$$||x||_2=1$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm([0.5**0.5,0.5**0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Orthogonal_:  \n",
    "$$x^Ty=0$$  \n",
    "If both are nonzero, this means that they are at a 90 degree angle to each other.  \n",
    "In $R^n$, at most n vectors may be mutually orthogonal with nonzero norm.  \n",
    "_Orthonormal_:  \n",
    "Unit and orthogonal.  \n",
    "_Orthogonal matrix_:  \n",
    "Square matrix whose rows are _mutually orthonormal_ and whose columns are _mutually orthonormal_: $A^TA=AA^T=I \\rightarrow A^{-1}=A^T$  \n",
    "their inverse is very cheap to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Eigendecomposition  \n",
    "We decompose a matrix into a set of eigenvectors and eigenvalues.  \n",
    "$$\n",
    "\\bf A v=\\lambda v\n",
    "$$  \n",
    "$\\bf A$ is the _eigenvector_ and the $\\lambda$ is known as the _eigenvalue_ corresponding to this eigenvector.  \n",
    "If $v$ is an eigenvector of $A$, then so is any rescaled vector $sv$, with the same eigenvalue, we usually only look for unit eigenvectors.  \n",
    "![](./pngs/2.7.1.png)  \n",
    "We can concatenate all of the eigenvectors to form a matrix $V$ with one eigenvector per column: $V=[v^{(1)},...,v^{(n)}]$ and $\\lambda = [\\lambda_1,...,\\lambda_n]^T$. The eigendecomposition of $A$ is then given by:  \n",
    "$$\n",
    "A=V\\ diag(\\lambda)\\ V^{-1}\n",
    "$$  \n",
    "Doing so can help us to analyze certain properties of the matrix.  \n",
    "Every real __symmetric__ matrix can be decomposed into an expression using only real-valued eigenvectros and eigenvalues.  \n",
    "$$\n",
    "A=Q\\ diag(Λ)\\ Q^{-1}\n",
    "$$  \n",
    "$Q$ is an orthogonal matrix composed of eigenvectors of $A$, and $Λ$ is a diagonal matrix. $Λ_{(i,i)}$ is associated with the eigenvector $Q_{:,i}$. We can think of $A$ as scaling space by $\\lambda_i$ in direction $v_{(i)}$  \n",
    "By convention, we usually sort the entries of Λ in descending order. Under this convention, the eigendecomposition is unique only if all of the eigenvalues are unique.  \n",
    "The matrix is singular if and only if any of the eigenvalues are zero.  \n",
    "_Positive semidefinite matrices_: they guarantee that $∀x, x^TAx ≥ 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.8 Singluar Value Decomposition\n",
    "The _singular value decomposition (SVD)_ factorize a martix into _singular vectors_ and _singular values_.  \n",
    "SVD is more generally applicable.  \n",
    "If a matrix is not square, the eigendecomposition is not defined  \n",
    "$$\n",
    "A=UDV^T\n",
    "$$  \n",
    "$A:m \\times n$  \n",
    "$U:m \\times m$  \n",
    "$D:m \\times n$  \n",
    "$V:n \\times n$  \n",
    "$U$ and $V$ are both defined to be orthogonal matrices, $D$ is defined to be a diagonal matrix, not necessarily square.  \n",
    "Element along the diagonal of $D$ are known as the _singular values_ of the $A$.  \n",
    "The columns of $U$ are known as the _left-singular vectors_, the columns of $V$ are known as the _right-singular vectors_.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 The Moore-Penrose Pseudoinverse\n",
    "$$\n",
    "Ax=y\\\\\n",
    "x=By\n",
    "$$  \n",
    "If $A$ is taller than it is wide, then it is possible for this equation to have no solution. If $A$ is wider than it is tall, then there could be multiple possible solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 The Trace Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 The Determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 Example: Principal Components Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
